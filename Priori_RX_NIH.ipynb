{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Bibliotecas PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Bibliotecas Torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "from torchvision.models import efficientnet_b7, EfficientNet_B7_Weights\n",
        "\n",
        "# Outras\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import copy\n",
        "import random\n",
        "import zipfile\n",
        "from google.colab import drive\n",
        "\n",
        "# Métricas Sklearn para avaliação\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix"
      ],
      "metadata": {
        "id": "bjTrCpgFkNun"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Passo 1: Acessar o Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eH9mo3hPjwfb",
        "outputId": "323bf59f-1c28-451c-8e68-aac157661329"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Montar o dataset, separando em treino, validação e teste\n",
        "train_dir ='/content/drive/MyDrive/Priori-RX/dataset_NIH/train'\n",
        "val_dir = '/content/drive/MyDrive/Priori-RX/dataset_NIH/validation'\n",
        "test_dir = '/content/drive/MyDrive/Priori-RX/dataset_NIH/test'"
      ],
      "metadata": {
        "id": "J2THTfNopZA-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data argumentation nos conjuntos de treino, validação e treino\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomRotation(20),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "        transforms.RandomGrayscale(p=0.1),\n",
        "        transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'test': transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}"
      ],
      "metadata": {
        "id": "cp7-6NnKmFOB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregar dataset com ImageFolder\n",
        "image_datasets = {\n",
        "    'train': datasets.ImageFolder(train_dir, transform=data_transforms['train']),\n",
        "    'val': datasets.ImageFolder(val_dir, transform=data_transforms['val']),\n",
        "    'test': datasets.ImageFolder(test_dir, transform=data_transforms['test'])\n",
        "}"
      ],
      "metadata": {
        "id": "SxEcq_q6nsNK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evitar multiprocessamento\n",
        "batch_size = 32\n",
        "dataloaders = {\n",
        "    'train': torch.utils.data.DataLoader(image_datasets['train'], batch_size=batch_size, shuffle=True, num_workers=0),\n",
        "    'val': torch.utils.data.DataLoader(image_datasets['val'], batch_size=batch_size, shuffle=True, num_workers=0),\n",
        "    'test': torch.utils.data.DataLoader(image_datasets['test'], batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "}"
      ],
      "metadata": {
        "id": "5tTwe0weoE5p"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obter classes encontradas e quantidade total de imagens por conjunto\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val', 'test']}\n",
        "class_names = image_datasets['train'].classes\n",
        "print('Classes encontradas', class_names)\n",
        "print('Total de imagens encontradas por conjunto: ', dataset_sizes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbIYHxYuoNTA",
        "outputId": "49f99a92-e8cb-4f6c-b237-52d2b964494b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes encontradas ['Doente', 'Normal']\n",
            "Total de imagens encontradas por conjunto:  {'train': 17513, 'val': 7486, 'test': 40}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregando o modelo pré-treinado EfficientNet-B7 - possui 7 blocos principais em model.features\n",
        "model = efficientnet_b7(weights=EfficientNet_B7_Weights.IMAGENET1K_V1)\n",
        "\n",
        "# Congelando camadas\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "for param in model.features[6:].parameters():  # deixando somente os dois últimos blocos descongelados\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Modificando as últimas camadas e incrementando um dropout\n",
        "num_ftrs = model.classifier[1].in_features  # Obtendo os recursos de entrada da camada final\n",
        "model.classifier = nn.Sequential(\n",
        "    nn.Flatten(),\n",
        "    nn.Dropout(0.5),      # drouptou para regularização\n",
        "    nn.Linear(num_ftrs, 512),  # camada intermidária\n",
        "    nn.ReLU(),                 # Activation function\n",
        "    nn.Dropout(0.3),      # outro dropout\n",
        "    nn.Linear(512, 2)     # camada final ajustada para classificação binária\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWYVuXvhoQwU",
        "outputId": "44477028-5417-482e-a47e-8eb91421b2bc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/efficientnet_b7_lukemelas-c5b4e57e.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b7_lukemelas-c5b4e57e.pth\n",
            "100%|██████████| 255M/255M [00:02<00:00, 121MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Função de perda com suavização de rótulo\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "\n",
        "# Definir otimizador com maior redução de peso\n",
        "optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4, weight_decay=5e-4)\n",
        "\n",
        "# ajuste da taxa de aprendizado com base na função cosseno\n",
        "scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
        "\n",
        "# Definindo o Early stopping e checkpoints\n",
        "patience = 5\n",
        "epochs_without_improvement = 0\n",
        "best_val_loss = float('inf')\n",
        "best_model_wts = copy.deepcopy(model.state_dict())"
      ],
      "metadata": {
        "id": "gM7fyQN8oSR-"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Definir o device (GPU se disponível, caso contrário, CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Mover o modelo para o dispositivo\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "AAbLeCOAs2YI"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop de treinamento com acumulação de gradiente\n",
        "num_epochs = 100\n",
        "accumulation_steps = 4  # acumula o gradiente a cada 4 batchs\n",
        "train_losses, val_losses = [], []\n",
        "train_accuracies, val_accuracies = [], []  # Armazenar previsões\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "\n",
        "    # Fase de treino\n",
        "    for i, (images, labels) in enumerate(tqdm(dataloaders['train'], desc=f\"Epoch {epoch+1}/{num_epochs}\")):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Cálculo Backward e perda\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels) / accumulation_steps  # Normalize loss by accumulation steps\n",
        "\n",
        "        # Backward\n",
        "        loss.backward()\n",
        "\n",
        "        if (i + 1) % accumulation_steps == 0:\n",
        "            optimizer.step()  # Atualizar os pesos depois do acumulo de gradiente\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        running_loss += loss.item() * labels.size(0)\n",
        "\n",
        "         # Cálculo da acurácia\n",
        "        _, preds = torch.max(outputs, 1)  # Obter classe prevista\n",
        "        correct_train += (preds == labels).sum().item()\n",
        "        total_train += labels.size(0)\n",
        "\n",
        "    train_loss = running_loss / dataset_sizes['train']\n",
        "    train_acc = correct_train / total_train  # Acurácia do treino\n",
        "    train_losses.append(train_loss)\n",
        "    train_accuracies.append(train_acc)\n",
        "\n",
        "    # Fase de validação\n",
        "    model.eval()\n",
        "    val_running_loss = 0.0\n",
        "    correct_val = 0\n",
        "    total_val = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in dataloaders['val']:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_running_loss += loss.item() * labels.size(0)\n",
        "\n",
        "            # Cálculo da acurácia\n",
        "            _, preds = torch.max(outputs, 1)  # Obter classe prevista\n",
        "            correct_val += (preds == labels).sum().item()\n",
        "            total_val += labels.size(0)\n",
        "\n",
        "    val_loss = val_running_loss / dataset_sizes['val']\n",
        "    val_acc = correct_val / total_val  # Acurácia da validação\n",
        "    val_losses.append(val_loss)\n",
        "    val_accuracies.append(val_acc)\n",
        "\n",
        "    # Checando o Early stopping - baseado no loss\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_model_wts = copy.deepcopy(model.state_dict())\n",
        "        epochs_without_improvement = 0\n",
        "    else:\n",
        "        epochs_without_improvement += 1\n",
        "        if epochs_without_improvement == patience:\n",
        "            print(\"Early stopping ativado.\")\n",
        "            break\n",
        "\n",
        "    # Ajuste da taxa de aprendizado\n",
        "    scheduler.step()\n",
        "\n",
        "    # Print dos resultados por época\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} - Train Acc: {train_acc:.4f} - Train Loss: {train_loss:.4f} x Val Acc: {val_acc:.4f} - Val Loss: {val_loss:.4f}, \")\n",
        "\n",
        "\n",
        "# Carregar modelo com melhores pesos\n",
        "model.load_state_dict(best_model_wts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "nXZigU1soUCb",
        "outputId": "d187f4e4-a442-405a-cb71-750ca7b8cd3a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/100:   0%|          | 2/548 [01:06<5:01:02, 33.08s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-274359cd9087>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# Fase de treino\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"Epoch {epoch+1}/{num_epochs}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    243\u001b[0m         \"\"\"\n\u001b[1;32m    244\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mdefault_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maccimage_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpil_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mpil_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;31m# open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3478\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3480\u001b[0;31m     \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3482\u001b[0m     \u001b[0mpreinit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotando gráficos com resultados\n",
        "plt.plot(train_losses, label='Training Loss', color='blue', linestyle='-', marker='o', markersize=4)\n",
        "plt.plot(val_losses, label='Validation Loss', color='red', linestyle='--', marker='x', markersize=4)\n",
        "plt.plot(train_accuracies, label='Training Accuracy', color='green', linestyle='-', marker='o', markersize=4)\n",
        "plt.plot(val_accuracies, label='Validation Accuracy', color='yellow', linestyle='--', marker='x', markersize=4)\n",
        "\n",
        "plt.title('Training and Validation over Epochs', fontsize=16)\n",
        "plt.xlabel('Epochs', fontsize=14)\n",
        "plt.ylabel('Parametros', fontsize=14)\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QAzpwwG8oVeD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# função para carregar e processar imagem\n",
        "def load_and_preprocess_image(image_path, transform):\n",
        "    \"\"\"\n",
        "    Carrega uma imagem, converte para RGB, aplica transformações e transforma num tensor para servir de entrada pro modelo treinado.\n",
        "\n",
        "    Argumentos da função:\n",
        "        image_path (str): caminho da imagem a ser carregada.\n",
        "        transform (torchvision.transforms.Compose): transformações aplicadas à imagem.\n",
        "\n",
        "    Retorno:\n",
        "        tuple: imagem original e tensor pré-processado pronto para entrar no modelo.\n",
        "    \"\"\"\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    return image, transform(image).unsqueeze(0)  # Aplicar transformações e adicionar dimensão de lote\n",
        "\n",
        "# Função de predição com carregamento, pré-processamento e predição\n",
        "def predict_image(model, image_path, transform, device):\n",
        "    \"\"\"\n",
        "    Carrega e processa a imagem, usando o modelo para prever a classe presente na imagem.\n",
        "\n",
        "    Argumentos:\n",
        "        model (torch.nn.Module): modelo de rede treinado .\n",
        "        image_path (str): caminho da imagem a ser analisada.\n",
        "        transform (torchvision.transforms.Compose): transformações que serão aplicadas a imagem.\n",
        "        device (torch.device): dispositivo que vai rodar o modelo (CPU ou GPU).\n",
        "\n",
        "    Retorno:\n",
        "        tuple: imagem original e classe prevista (como uma matriz numpy).\n",
        "    \"\"\"\n",
        "    original_image, image_tensor = load_and_preprocess_image(image_path, transform)\n",
        "    model.eval()  # colocando o modelo no modo de avaliação\n",
        "    with torch.no_grad():  # Desabilitar cálculo de gradiente para inferência\n",
        "        image_tensor = image_tensor.to(device)  # muda tensor para o dispositivo indicado\n",
        "        outputs = model(image_tensor)           # obter a saída do modelo\n",
        "        probabilities = F.softmax(outputs, dim=1)  # aplicando softmax para obter a  classe prevista\n",
        "\n",
        "    return original_image, probabilities.cpu().numpy().flatten()\n",
        "\n",
        "\n",
        "# Função para visualizar as previsões\n",
        "def visualize_predictions(image, probabilities, class_names, top_k=2):\n",
        "    \"\"\"\n",
        "    Exibe a imagem original ao lado de um gráfico de barras das previsões.\n",
        "\n",
        "    Argumentos:\n",
        "        image (PIL.Image): imagem original\n",
        "        probabilities (np.array): Matriz de probabilidades de classe geradas pelo modelo.\n",
        "        class_names (list): Lista de nomes de classes correspondentes à saída do modelo.\n",
        "        top_k (int): Número das principais previsões a serem exibidas (o padrão é 2 para classificação binária).\n",
        "    \"\"\"\n",
        "    top_k_idx = probabilities.argsort()[-top_k:][::-1]         # Obtendo o indice de cada previsão\n",
        "    top_classes = [class_names[i] for i in top_k_idx]          # Recuperar os nomes de classes\n",
        "    top_probabilities = probabilities[top_k_idx]               # Obter probabilidades para as principais previsões\n",
        "\n",
        "    # Plotando a imagem e suas previsões\n",
        "    fig, axarr = plt.subplots(1, 2, figsize=(12, 6))\n",
        "    axarr[0].imshow(image)\n",
        "    axarr[0].axis(\"off\")\n",
        "\n",
        "    axarr[1].barh(top_classes, top_probabilities)\n",
        "    axarr[1].set_xlabel(\"Probability\")\n",
        "    axarr[1].set_title(\"Top Class Predictions\")\n",
        "    axarr[1].invert_yaxis()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Função para selecionar uma imagem aleatória da pasta de teste\n",
        "def get_random_image_from_folder(folder_path):\n",
        "    \"\"\"\n",
        "    Seleciona uma imagem aleatória do conjunto de teste\n",
        "\n",
        "    Argumentos:\n",
        "        folder_path (str): caminho da pasta \"teste\"\n",
        "\n",
        "    Retorno:\n",
        "        tuple: caminho para a imagem selecionada.\n",
        "    \"\"\"\n",
        "    classes = os.listdir(folder_path)\n",
        "    random_class = random.choice(classes)\n",
        "    class_folder = os.path.join(folder_path, random_class)\n",
        "    image_files = os.listdir(class_folder)\n",
        "    random_image_file = random.choice(image_files)\n",
        "    return os.path.join(class_folder, random_image_file), random_class\n",
        "\n",
        "# Prevendo 10 imagens aleatórias\n",
        "test_folder_path = test_dir\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),    # redimensionando as imagens para ficar de acordo com o modelo\n",
        "    transforms.ToTensor(),            # Convertendo imagem para tensor\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # Normalizando de acordo com os padrões ImageNet\n",
        "])\n",
        "\n",
        "class_names = [\"NORMAL\", \"DOENTE\"]  # definindo as classes\n",
        "\n",
        "# Loop para analisar uma quantidade n de imagens aleatórias e ver resultados\n",
        "n = 10\n",
        "for i in range(n):\n",
        "    test_image_path, actual_class = get_random_image_from_folder(test_folder_path)  # Selecionar imagem\n",
        "    original_image, probabilities = predict_image(model, test_image_path, transform, device)  # Fazer predição\n",
        "    visualize_predictions(original_image, probabilities, class_names)  # Visualizar resultados\n",
        "\n",
        "    # Exibir classe real e classe prevista\n",
        "    predicted_class_idx = probabilities.argmax()\n",
        "    predicted_class = class_names[predicted_class_idx]\n",
        "    print(f\"Imagem {i+1}:\")\n",
        "    print(f\"  Classe real: {actual_class}\")\n",
        "    print(f\"  Classe prevista: {predicted_class}\")\n",
        "    print(\"-\" * 30)"
      ],
      "metadata": {
        "id": "Mz2mYpk8oVzK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Função de avaliação do modelo\n",
        "def evaluate_model(model, dataloader, class_names):\n",
        "    model.eval() # colocando o modelo no modo de avaliação\n",
        "    running_corrects = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            # Obtendo todas as previsões e classes reais\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Calculando acurácia\n",
        "    accuracy = running_corrects.double() / len(dataloader.dataset) * 100\n",
        "    print(f'Test Accuracy: {accuracy:.4f}%')\n",
        "\n",
        "    # Calculando previsão, recall, and F1-score\n",
        "    precision = precision_score(all_labels, all_preds, average='binary')\n",
        "    recall = recall_score(all_labels, all_preds, average='binary')\n",
        "    f1 = f1_score(all_labels, all_preds, average='binary')\n",
        "\n",
        "    print(f'Precisão: {precision:.4f}')\n",
        "    print(f'Recall: {recall:.4f}')\n",
        "    print(f'F1 Score: {f1:.4f}')\n",
        "\n",
        "    # Montar matriz de confusão\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(cm)\n",
        "\n",
        "    # Exibir matriz de confusão\n",
        "    fig, ax = plt.subplots(figsize=(6, 6))\n",
        "    ax.matshow(cm, cmap=plt.cm.Blues, alpha=0.6)\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            ax.text(x=j, y=i, s=cm[i, j], va='center', ha='center')\n",
        "\n",
        "    plt.xlabel('Classes previstas')\n",
        "    plt.ylabel('Classes verdadeiras')\n",
        "    plt.title('Matriz de confusão')\n",
        "    plt.xticks(ticks=[0, 1], labels=class_names)\n",
        "    plt.yticks(ticks=[0, 1], labels=class_names)\n",
        "    plt.show()\n",
        "\n",
        "# Example usage:\n",
        "class_names = [\"NORMAL\", \"DOENTE\"]\n",
        "evaluate_model(model, dataloaders['test'], class_names)"
      ],
      "metadata": {
        "id": "vLyVZ1cMoXcu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Após o treinamento\n",
        "model.load_state_dict(best_model_wts)\n",
        "\n",
        "# Caminho para salvar os pesos\n",
        "path_to_save = \"best_model_weights.pth\"\n",
        "\n",
        "# Salvar os pesos\n",
        "torch.save(best_model_wts, path_to_save)\n",
        "print(f\"Pesos do melhor modelo salvos em {path_to_save}\")\n"
      ],
      "metadata": {
        "id": "VgSLStNNoZh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = \"checkpoint.pth\"\n",
        "\n",
        "# Salvar checkpoint\n",
        "torch.save({\n",
        "    'model_state_dict': best_model_wts,\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'scheduler_state_dict': scheduler.state_dict(),\n",
        "    'epoch': epoch,\n",
        "    'train_losses': train_losses,\n",
        "    'val_losses': val_losses\n",
        "}, checkpoint_path)\n",
        "print(f\"Checkpoint salvo em {checkpoint_path}\")\n"
      ],
      "metadata": {
        "id": "x_Pp-8hzzqfW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}